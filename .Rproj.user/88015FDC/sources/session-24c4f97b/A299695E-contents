# =============================================================================
# MODIFIED ALCOVE-RL MODEL WITH ASYMMETRIC LEARNING RATES
# =============================================================================
# 
# Based on: Alexander S. Rich, & Todd M. Gureckis 2018
# Original ALCOVE model: Kruschke (1992)
# 
# This implementation extends standard ALCOVE with:
# - Asymmetric learning rates for gains vs losses (lw_plus, lw_minus)
# - Multiple teacher types (strict, contingent, humble)
# - Reinforcement learning framework for approach/avoidance decisions
#
# Key modification: Separate learning rates for positive outcomes (gains) 
# and negative outcomes (losses) to capture asymmetric learning effects
# =============================================================================

# Initial parameter examples (commented out):
# h = matrix(c(0,0,0,1,1,0,1,1), byrow = T, 4, 2)  # Exemplar representations (Dim x Exemplar)
# w = matrix(c(0,0,0,0,0,0,0,0), byrow = T, 2, 4)  # Association weights (Category x Exemplar)
# alpha = c(0.5, 0.5)                               # Attention weights for each dimension
# lamda_w = 0.75                                    # Learning rate for association weights

# =============================================================================
# SIMILARITY/DISTANCE CALCULATION (Kruschke 1992: Equation 1)
# =============================================================================
# Calculates activation of hidden nodes based on similarity to stored exemplars
# Uses city-block distance with attention weighting
#
# Parameters:
#   h: matrix (Dim x Exemplar) - stored exemplar representations
#   a_in: vector - input stimulus dimensions
#   alpha: vector - attention weights for each dimension (learnable)
#   r: numeric - distance metric parameter
#   q: numeric - normalisation parameter for distance
#   c: numeric - specificity/generalisation parameter (higher = more specific)
#
# Returns:
#   a_hid: vector - activation values for each hidden node (exemplar)
# Liu et al commented settings: r=1, c=2
# =============================================================================

sim_distance = function(h, a_in, alpha, r = 2,q = 1, c = 2){
  a_hid = rep(0, dim(h)[2])
  for(j in 1: dim(h)[2]){ #j: stim index
    for(i in 1: length(a_in)){ #i: dimension index
      a_hid[j] = a_hid[j] + alpha[i]*((abs(h[i,j] - a_in[i]))^r)
    }
  }
  a_hid = exp(-c*a_hid^(q/r))
  return(a_hid)
}

# =============================================================================
# OUTPUT CALCULATION (Kruschke 1992: Equation 2, modified)
# =============================================================================
# Computes output node activations from hidden node activations
# Uses learned association weights to map exemplars to categories
#
# Parameters:
#   a_hid: vector - hidden node activations from sim_distance()
#   w: matrix (Category x Exemplar) - association weights (learnable)
#
# Returns:
#   a_o: vector - output activations for each category
# =============================================================================

out_put = function(a_hid, w){
  a_o = rep(0, dim(w)[1])
  for(k in 1: dim(w)[1]){ # Category index
    for(j in 1:length(a_hid)){
      a_o[k] = a_o[k] + w[k,j]*a_hid[j]
    }
  }
  a_o = a_o#/sum(a_hid) #normalisation as shown in Gureckis paper; in original ALCOVE no normalisation
  return(a_o)
}

# =============================================================================
# RESPONSE PROBABILITY (Kruschke 1992: Equation 3)
# =============================================================================
# Converts output activations to response probabilities using Luce choice rule
# Implements softmax function with temperature parameter
#
# Parameters:
#   a_o: vector - output activations from out_put()
#   phi: numeric - response determinism (higher = more deterministic) 
#        (Liu et al commented phi=1)
#
# Returns:
#   pr: vector - probability of choosing each category
# =============================================================================

rep_prob = function(a_o, phi){
  pr = exp (phi * a_o)/sum(exp(phi*a_o))
  return(pr)
}

# =============================================================================
# CONTINGENT TEACHER (Todd Gureckis)
# =============================================================================
# Provides teaching signal only for incorrect responses
# Allows model to learn from mistakes while not updating correct responses
#
# Parameters:
#   a_o: vector - output activations
#   t: vector - target teaching signal (1 for correct, NA for no feedback)
#
# Returns:
#   teacher: vector - modified teaching signal
# =============================================================================

contingent_teacher = function(a_o, t){
  teacher = t 
  teacher[which(is.na(t))] = a_o[which(is.na(t))] #if feedback is not available, no update
  return(teacher)
}

# =============================================================================
# HUMBLE TEACHER
# =============================================================================
# Provides bounded teaching signals to prevent overcorrection
# Ensures teaching signal doesn't exceed reasonable bounds
#
# Parameters:
#   a_o: vector - output activations
#   t: vector - target categories (1 for correct, 0 for incorrect)
#
# Returns:
#   teacher: vector - bounded teaching signal
# =============================================================================

humble_teacher = function(a_o, t){
  teacher = rep(NA,length(t))
  for (k in 1:length(t)){
    if (t[k] == 1){
      teacher[k] = max(c(1,a_o[k]))
    }else {
      teacher[k] = min(c(-1,a_o[k]))
    }
  }
  return(teacher)
}


# =============================================================================
# ERROR CALCULATION (Kruschke 1992: Equation 4a)
# =============================================================================
# Computes prediction error for weight updates
#
# Parameters:
#   teacher: vector - teaching signal (target)
#   a_o: vector - output activations (prediction)
#
# Returns:
#   e: vector - error signal for each output node
# =============================================================================

error = function(teacher, a_o){
  e = (teacher - a_o)
  return(e)
}

# =============================================================================
# ASSOCIATION WEIGHT UPDATE WITH ASYMMETRIC LEARNING
# =============================================================================
# KEY MODIFICATION: Uses different learning rates based on outcome valence
# Captures asymmetric learning from gains vs losses
#
# Parameters:
#   lamda_w_plus: numeric - learning rate for positive outcomes (gains)
#   lamda_w_minus: numeric - learning rate for negative outcomes (losses)
#   error: vector - prediction errors
#   a_hid: vector - hidden node activations
#   outcome_value: numeric - actual outcome (+1 gain, 0 neutral, -3 loss)
#
# Returns:
#   del_w: matrix - weight changes (Category x Exemplar)
# =============================================================================

delta_w_with_outcome = function(lamda_w_plus, lamda_w_minus, error, a_hid, outcome_value){
  del_w = matrix(0, length(error), length(a_hid))
  
  for(k in 1:length(error)){
    for(j in 1:length(a_hid)){
      # Use outcome value for asymmetry
      if(outcome_value > 0){  # Positive outcome (gained points)
        lr = lamda_w_plus
      } else {                 # Negative outcome (lost points or zero)
        lr = lamda_w_minus
      }
      del_w[k,j] = lr * error[k] * a_hid[j]
    }
  }
  return(del_w)
}

# =============================================================================
# BACKPROPAGATION OF ERROR TO HIDDEN NODES
# =============================================================================
# Propagates error signal back to hidden layer for attention updates
#
# Parameters:
#   a_hid: vector - hidden node activations
#   error: vector - output layer errors
#   w: matrix - current association weights
#
# Returns:
#   bp: vector - backpropagated error for each hidden node
# =============================================================================

bp_error = function(a_hid, error, w){
  bp = error %*% w
  for(j in 1: length(a_hid)){
    bp[j] = bp[j]*a_hid[j]
  }
  return(bp)
}

# =============================================================================
# ATTENTION WEIGHT UPDATE
# =============================================================================
# Updates dimensional attention weights based on backpropagated error
# Learns which stimulus dimensions are most relevant for categorisation
#
# Parameters:
#   lamda_alpha: numeric - learning rate for attention weights
#   c: numeric - specificity parameter
#   h: matrix - exemplar representations
#   a_in: vector - input stimulus
#   bp: vector - backpropagated errors
#
# Returns:
#   del_a: vector - attention weight changes for each dimension
# =============================================================================

#Double check this equation?
#lamda_alpha = 0.3
delta_alpha = function(lamda_alpha, c, h, a_in, bp){
  hmx = abs(h-a_in) # h(what's that?) minus dimensional inputs
  del_a = rep(0, length(a_in)) # initialise the weights associated with the dimensions
  for(j in 1:(dim(h)[2])){
    del_a = del_a + bp[j] * hmx[,j] *c
  }
  del_a = -lamda_alpha*del_a
  return(del_a)
}

# =============================================================================
# SINGLE TRIAL UPDATE
# =============================================================================
# Processes one trial: computes response, gets feedback, updates weights
#
# Parameters:
#   c: numeric - specificity parameter
#   phi: numeric - response determinism
#   lw_plus: numeric - learning rate for gains
#   lw_minus: numeric - learning rate for losses
#   la: numeric - attention learning rate
#   alpha: vector - current attention weights
#   w: matrix - current association weights
#   h: matrix - exemplar representations
#   q, r: numeric - distance metric parameters
#   a_in: vector - input stimulus
#   fb: vector - feedback (teaching signal)
#   t_type: numeric - teacher type (1=strict, 2=contingent, 3=humble)
#
# Returns:
#   list containing:
#     pr: response probabilities
#     w: updated association weights
#     alpha: updated attention weights
#     e: prediction errors
# =============================================================================

ALCOV_trial = function(c, phi, lw_plus, lw_minus, la, alpha, w, h, q, r, a_in, fb, t_type){
  # 1) FORWARD PASS: compute current trial performance
  a_hid = sim_distance(h, a_in, alpha, c)
  a_o = out_put(a_hid, w)
  pr =  rep_prob(a_o, phi)

  # 2) LEARNING: update weights from feedback
  # Select teacher type
  if(t_type == 1){  # Strict teacher - always provides full feedback
    t = fb
  } else if(t_type == 2){  # Contingent teacher - feedback only on errors
    t = contingent_teacher(a_o, fb)
  } else if(t_type == 3){  # Humble teacher - bounded feedback
    t = humble_teacher(a_o, fb)
  }

  # Calculate error
  e = error(t, a_o)

  # DETERMINE OUTCOME VALUE for asymmetric learning
  # Based on which action would be taken (highest probability)
  predicted_action = which.max(pr)
  
  if(predicted_action == 1){  # Would approach
    # +1 if friendly (gain), -3 if dangerous (loss)
    outcome_value = ifelse(fb[1] == 1, 1, -3)
  } else {  # Would avoid
    outcome_value = 0  # No points for avoidance
  }
  
  # UPDATE WEIGHTS with outcome-dependent learning rates
  dw = delta_w_with_outcome(lw_plus, lw_minus, e, a_hid, outcome_value)
  
  # Update attention weights
  bp = bp_error(a_hid, e, w)
  da = delta_alpha(la, c, h, a_in, bp)
  
  # Apply weight updates
  nw = w + dw
  nalpha = alpha + da
  nalpha[which(nalpha < 0)] = 0  # Enforce non-negative attention (Wills, 1996)
  
  # Store results
  t_store = list()
  t_store$pr = pr
  t_store$w = nw
  t_store$alpha = nalpha
  t_store$e = e
  return(t_store)
  
}

# =============================================================================
# MAIN ALCOVE-RL FUNCTION
# =============================================================================
# Runs full ALCOVE-RL model across all trials in dataset
#
# Parameters:
#   intl_par: vector of 5 parameters [c, phi, lw_plus, lw_minus, la]
#   tr_data: matrix - trial data (dimensions, feedback, control)
#   h: matrix - exemplar representations
#   w: matrix - initial association weights
#   alpha: vector - initial attention weights
#   q, r: numeric - distance metric parameters
#   exp_info: vector [d_num, k_num, stim_num, t_type]
#     d_num: number of stimulus dimensions
#     k_num: number of categories
#     stim_num: number of exemplars
#     t_type: teacher type
#
# Returns:
#   list containing:
#     pr: matrix of response probabilities for all trials
#     w: final association weights
#     a: matrix of attention weights across trials
#     e: matrix of errors across trials
#     w_list: matrix of association weights across trials
# =============================================================================

ALCOVE_RL = function(intl_par, tr_data, h, w, alpha, q, r, exp_info){
  # Parse parameters (5 parameters for asymmetric learning)
  c = intl_par[1]         # Specificity
  phi = intl_par[2]       # Response determinism
  lw_plus = intl_par[3]   # Learning rate for positive outcomes (gains)
  lw_minus = intl_par[4]  # Learning rate for negative outcomes (losses)
  la = intl_par[5]        # Attention learning rate

  # Parse experiment information
  d_num = exp_info[1]     # Number of dimensions
  k_num = exp_info[2]     # Number of categories
  stim_num = exp_info[3]  # Number of exemplars
  t_type = exp_info[4]    # Teacher type

  # Initialise storage matrices
  trial_num = dim(tr_data)[1]
  pr_store = matrix(NA, trial_num, 2)               # Response probabilities
  alpha_store = matrix(NA, trial_num, d_num)        # Attention weights
  e_store = matrix(NA, trial_num, k_num)            # Errors
  w_store = matrix(NA, trial_num, (k_num*stim_num)) # Association weights
  
  # Process each trial
  
  for(n in 1: trial_num){
    trial = tr_data[n, ]
    a_in = as.numeric(trial[1:d_num])
    fb = as.numeric(trial[(d_num+1):(d_num+k_num)])
    ctr = as.numeric(trial[(d_num+k_num+1)])

    if (ctr == 0){ # TRAINING TRIAL
      # Store current weights
      alpha_store[n,] = alpha
      w_store[n, ] = as.numeric(t(w))
      # Run trial with learning
      trial_fit = ALCOV_trial(c, phi, lw_plus, lw_minus, la, alpha, w, h, q, r, a_in, fb, t_type)
      # Update weights for next trial
      w = trial_fit$w
      alpha = trial_fit$alpha
      # Store results
      pr_store[n, 1:2] = trial_fit$pr
      e_store[n,] = trial_fit$e
    } else if (ctr == 2){ # TEST TRIAL (no learning)
      # Store current weights
      alpha_store[n,] = alpha
      w_store[n, ] = as.numeric(t(w))
      # Run trial WITHOUT learning (learning rates set to 0)
      trial_fit = ALCOV_trial(c, phi, lw_plus=0, lw_minus=0, la=0, alpha, w, h, q, r, a_in, fb, t_type)
      # Store results (weights not updated)
      pr_store[n, 1:2] = trial_fit$pr
      e_store[n,] = trial_fit$e
    }
  }

  # Package results
  store = list()
  store$pr = pr_store      # Response probabilities
  store$w = w              # Final weights
  store$a = alpha_store    # Attention weight trajectory
  store$e = e_store        # Error trajectory
  store$w_list = w_store   # Association weight trajectory

  return(store)
}

# =============================================================================
# NEGATIVE LOG-LIKELIHOOD FUNCTION (for optimisation with optim)
# =============================================================================
# Computes negative log-likelihood of model given data
# Uses sigmoid transformation to enforce parameter bounds
#
# Parameters:
#   int_pars: vector - unconstrained parameters (will be transformed)
#   data: dataframe - experimental data with responses
#   h, w, alpha: model structure parameters
#   q, r: distance metric parameters
#   exp_info: experiment information
#   upperbound: vector - upper bounds for each parameter
#
# Returns:
#   nlk: numeric - negative log-likelihood
# =============================================================================

ALCOVE_nlk = function(int_pars, data, h, w, alpha, q, r, exp_info, upperbound){
  d_num = exp_info[1] 
  k_num = exp_info[2]
  stim_num = exp_info[3]
  t_type = exp_info[4]
  
  tr_data = data[, 1:(d_num+k_num+1)]
  rep = data$rep
  new_param = rep(NA, length(int_pars))  # Now handles 5 parameters
  for(i in 1:length(int_pars)){
    new_param[i] = upperbound[i]/(1+exp(-int_pars[i]))
  }
  m_fit = ALCOVE_RL(new_param, tr_data, h, w, alpha, q = 1, r = 2, exp_info) 
  prob = m_fit$pr
  log_prob = log(prob)
  llk = 0
  nlk = 0 
  for(i in 1:length(rep)){
    llk = llk + log_prob[i, rep[i]]
    nlk = nlk - log_prob[i, rep[i]]
  }
  return(nlk)
}

# =============================================================================
# NEGATIVE LOG-LIKELIHOOD FOR DEOPTIM
# =============================================================================
# Alternative NLK function for DEoptim optimiser (uses bounded parameters directly)
#
# Parameters:
#   int_pars: vector - bounded parameters (no transformation needed)
#   data: dataframe - experimental data
#   h, w, alpha: model structure
#   q, r: distance parameters
#   exp_info: experiment information
#
# Returns:
#   nlk: numeric - negative log-likelihood
# =============================================================================

ALCOVE_nlk_DEoptim = function(int_pars, data, h, w, alpha, q, r, exp_info){
  d_num = exp_info[1] 
  k_num = exp_info[2]
  stim_num = exp_info[3]
  t_type = exp_info[4]
  
  col_n = d_num+k_num+1
  tr_data = data[, 1:col_n]
  rep = data$rep
  new_param = int_pars  # Now expects 5 parameters
  
  m_fit = ALCOVE_RL(new_param, tr_data, h, w, alpha, q = 1, r = 2, exp_info) 
  prob = m_fit$pr
  prob[which(prob==0, arr.ind = T)] = 10^-10
  log_prob = log(prob)
  llk = 0
  nlk = 0 
  for(i in 1:length(rep)){
    llk = llk + log_prob[i, rep[i]]
    nlk = nlk - log_prob[i, rep[i]]
  }
  
  return(nlk)
}

# =============================================================================
# PARAMETER TRANSFORMATION HELPER
# =============================================================================
# Transforms unbounded parameters to bounded range using sigmoid
# Used after optimisation to get interpretable parameter values
#
# Parameters:
#   output_param: vector - unbounded parameters from optimiser
#   upperbound: vector - upper bounds for each parameter
#
# Returns:
#   new_param: vector - bounded parameters
# =============================================================================

transform_output = function(output_param, upperbound){
  new_param = rep(NA, length(output_param))  # Now handles 5 parameters
  for(i in 1:length(output_param)){
    new_param[i] = upperbound[i]/(1+exp(-output_param[i]))
  }
  return(new_param)
}
